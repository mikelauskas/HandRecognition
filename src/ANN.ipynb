{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from image import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load images and labels, split the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = Image.load_images(\"data1000\", flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(images, labels, random_state=15, shuffle=True, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data and transform labels into categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X_train.shape[1]\n",
    "num_classes = Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN():\n",
    "  # create model with 1 hidden layer\n",
    "    model = Sequential()\n",
    "    #Input and hidden layer\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    #Output layer\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                optimizer='adam', metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN_2():\n",
    "  # create model with 2 hidden layer\n",
    "    model = Sequential()\n",
    "    #Input and hidden layer\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    #adding one more layer\n",
    "    model.add(Dense(128,  kernel_initializer='normal', activation='relu'))\n",
    "    #Output layer\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                optimizer='adam', metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4480 samples, validate on 1494 samples\n",
      "Epoch 1/15\n",
      " - 7s - loss: 4.6462 - acc: 0.3661 - val_loss: 3.6253 - val_acc: 0.5127\n",
      "Epoch 2/15\n",
      " - 6s - loss: 3.3306 - acc: 0.6062 - val_loss: 3.3288 - val_acc: 0.6185\n",
      "Epoch 3/15\n",
      " - 5s - loss: 3.0355 - acc: 0.7246 - val_loss: 3.1509 - val_acc: 0.6961\n",
      "Epoch 4/15\n",
      " - 5s - loss: 2.9016 - acc: 0.7761 - val_loss: 3.1132 - val_acc: 0.7135\n",
      "Epoch 5/15\n",
      " - 6s - loss: 2.8453 - acc: 0.7946 - val_loss: 3.0282 - val_acc: 0.7383\n",
      "Epoch 6/15\n",
      " - 6s - loss: 2.7880 - acc: 0.8185 - val_loss: 3.0019 - val_acc: 0.7530\n",
      "Epoch 7/15\n",
      " - 6s - loss: 2.7653 - acc: 0.8241 - val_loss: 2.9849 - val_acc: 0.7610\n",
      "Epoch 8/15\n",
      " - 6s - loss: 2.7524 - acc: 0.8275 - val_loss: 2.9592 - val_acc: 0.7651\n",
      "Epoch 9/15\n",
      " - 6s - loss: 2.7369 - acc: 0.8310 - val_loss: 2.9585 - val_acc: 0.7684\n",
      "Epoch 10/15\n",
      " - 6s - loss: 2.7293 - acc: 0.8315 - val_loss: 2.9664 - val_acc: 0.7704\n",
      "Epoch 11/15\n",
      " - 6s - loss: 2.7260 - acc: 0.8319 - val_loss: 2.9608 - val_acc: 0.7624\n",
      "Epoch 12/15\n",
      " - 6s - loss: 2.7227 - acc: 0.8319 - val_loss: 2.9391 - val_acc: 0.7724\n",
      "Epoch 13/15\n",
      " - 6s - loss: 2.7184 - acc: 0.8321 - val_loss: 2.9401 - val_acc: 0.7691\n",
      "Epoch 14/15\n",
      " - 6s - loss: 2.7176 - acc: 0.8321 - val_loss: 2.9437 - val_acc: 0.7724\n",
      "Epoch 15/15\n",
      " - 5s - loss: 2.7147 - acc: 0.8321 - val_loss: 2.9411 - val_acc: 0.7751\n",
      "Final accuracy: 77.51%\n",
      "Baseline Error: 22.49%\n"
     ]
    }
   ],
   "source": [
    "# Training of the model with 1 hidden layer\n",
    "# The model is fit over 10 epochs with updates every 200 images.\n",
    "\n",
    "# build the model\n",
    "model = ANN()\n",
    "# Fit the model\n",
    "hist = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=15, batch_size=200, verbose=2)\n",
    "#The test data is used as the validation dataset, allowing you to see the skill of the model as it trains.\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Final accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4480 samples, validate on 1494 samples\n",
      "Epoch 1/15\n",
      " - 7s - loss: 1.6727 - acc: 0.3598 - val_loss: 1.2833 - val_acc: 0.5013\n",
      "Epoch 2/15\n",
      " - 6s - loss: 1.0503 - acc: 0.6223 - val_loss: 0.9534 - val_acc: 0.6466\n",
      "Epoch 3/15\n",
      " - 6s - loss: 0.7138 - acc: 0.7674 - val_loss: 0.6889 - val_acc: 0.7704\n",
      "Epoch 4/15\n",
      " - 6s - loss: 0.5151 - acc: 0.8355 - val_loss: 0.5598 - val_acc: 0.7965\n",
      "Epoch 5/15\n",
      " - 6s - loss: 0.3545 - acc: 0.8949 - val_loss: 0.4707 - val_acc: 0.8333\n",
      "Epoch 6/15\n",
      " - 7s - loss: 0.2399 - acc: 0.9335 - val_loss: 0.3761 - val_acc: 0.8728\n",
      "Epoch 7/15\n",
      " - 7s - loss: 0.1752 - acc: 0.9562 - val_loss: 0.3677 - val_acc: 0.8735\n",
      "Epoch 8/15\n",
      " - 6s - loss: 0.1221 - acc: 0.9721 - val_loss: 0.2990 - val_acc: 0.8956\n",
      "Epoch 9/15\n",
      " - 6s - loss: 0.0742 - acc: 0.9895 - val_loss: 0.2778 - val_acc: 0.9076\n",
      "Epoch 10/15\n",
      " - 6s - loss: 0.0573 - acc: 0.9906 - val_loss: 0.2469 - val_acc: 0.9190\n",
      "Epoch 11/15\n",
      " - 6s - loss: 0.0401 - acc: 0.9955 - val_loss: 0.2513 - val_acc: 0.9210\n",
      "Epoch 12/15\n",
      " - 6s - loss: 0.0263 - acc: 0.9989 - val_loss: 0.2383 - val_acc: 0.9210\n",
      "Epoch 13/15\n",
      " - 6s - loss: 0.0179 - acc: 0.9996 - val_loss: 0.2242 - val_acc: 0.9244\n",
      "Epoch 14/15\n",
      " - 6s - loss: 0.0128 - acc: 0.9996 - val_loss: 0.2218 - val_acc: 0.9311\n",
      "Epoch 15/15\n",
      " - 6s - loss: 0.0091 - acc: 1.0000 - val_loss: 0.2171 - val_acc: 0.9317\n",
      "Final accuracy: 93.17%\n",
      "Baseline Error: 6.83%\n"
     ]
    }
   ],
   "source": [
    "# Training of the model with 2 hidden layers\n",
    "# The model is fit over 10 epochs with updates every 200 images.\n",
    "\n",
    "# build the model\n",
    "model_2 = ANN_2()\n",
    "# Fit the model\n",
    "hist_2 = model_2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=15, batch_size=200, verbose=2)\n",
    "# The test data is used as the validation dataset, allowing you to see the skill of the model as it trains.\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model_2.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Final accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the model for prediction on another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-a42959bef967>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_old\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mY_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[1;32m-> 2320\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# small reductions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = Image.load_images(\"data_old\", flatten=True)\n",
    "X_val = X_val.astype('float32') / 255\n",
    "Y_val = np_utils.to_categorical(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = model.predict(X_val)\n",
    "Y_predict_2 = model_2.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(Y_predict.argmax(axis=1) == Y_val.argmax(axis=1)) / Y_val.shape[0]\n",
    "accuracy_2 = sum(Y_predict_2.argmax(axis=1) == Y_val.argmax(axis=1)) / Y_val.shape[0]\n",
    "print(\"Accuracy with the 1st model : %.2f%%\" % (accuracy*100))\n",
    "print(\"Accuracy with the 2nd model : %.2f%%\" % (accuracy_2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the evolution of the accuracy \n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "ax1.plot(hist.history['acc'], label='Acc.')\n",
    "ax1.plot(hist.history['val_acc'], label='Validation acc.')\n",
    "ax1.axhline(accuracy, ls='--', c='r', label='Test acc.')\n",
    "ax1.set_ylim([0,1])\n",
    "ax1.set_xlabel('Training step')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('ANN with 1 hidden layer')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist_2.history['acc'], label='acc')\n",
    "ax2.plot(hist_2.history['val_acc'], label='Validation acc.')\n",
    "ax2.axhline(accuracy_2, ls='--', c='r', label='Test acc.')\n",
    "ax2.set_ylim([0,1])\n",
    "ax2.set_xlabel('Training step')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('ANN with 2 hidden layers')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Live test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with one hidden layer in live. Stay still for the prediction.\n",
    "# \n",
    "# Press B to select a background\n",
    "# Press SPACE to return an image\n",
    "# Press ESC to shut the window\n",
    "# Press R or L to move the window to the left or right\n",
    "# Press P or M to increase or decrease the size of the window\n",
    "\n",
    "img = Image.captureImage(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Returned image\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "plt.show\n",
    "print(\"Prediction :\", model.predict(np.array([img.flatten()])).argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
